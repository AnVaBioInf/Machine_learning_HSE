# -*- coding: utf-8 -*-
"""Копия блокнота "HW3.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j9HbCOOyhrO4c-BzoVHV30SrPhe9j3d2

Dataset used: Red wine quality (https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv) \

Input variables (based on physicochemical tests):\
1 - fixed acidity\
2 - volatile acidity\
3 - citric acid\
4 - residual sugar\
5 - chlorides\
6 - free sulfur dioxide\
7 - total sulfur dioxide\
8 - density\
9 - pH\
10 - sulphates\
11 - alcohol\
Output variable (based on sensory data):\
12 - quality (score between 0 and 10)\

## Downloading libraries and data
"""

# importing libraries
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
from math import sqrt

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn import metrics

# loading the dataset
!wget -O winequality.csv 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'

# reading data into df
df = pd.read_csv("winequality.csv", sep=';')

"""## Exploring the data"""

df.head()

print(df.shape)

print(df.columns)

# Checking type of data and % of missing values
df.info()
df.isnull().sum()

"""Conclusion: There are no missing values in out data set

### Checking representetion of different classes in the data set
"""

df['quality'].value_counts()

df['quality'].value_counts().plot(kind='bar')
plt.title('Class Frequency Table')
plt.xlabel('Class Type')
plt.ylabel('Counts')

"""Conclusion: clasess are disbalanced

### Checking summary of numerical data
"""

df.describe()

"""Conclusion: Citric acid, recidual sugar, chlorides, dioxide, sulfur dioxide have high std (around +-50% or more)

### Checking type of distribution of the data
"""

df.hist(bins=20, figsize=(12,12))

df.skew()

"""Conclusion: some destributions are scewed

### Checking correletion between the features and noisiness
"""

# Building a correlation matrix
corr_matrix = df.corr()
corr_matrix

plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix,cmap='YlGnBu',annot=True);

"""### Checking the data for outliers"""

fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 10))
for idx, feature in  enumerate(df.columns[:6]):
    sns.boxplot(x = 'quality', y = feature, data=df, ax=axes[int(idx / 3), idx % 3])
    axes[int(idx / 3), idx % 3].set_xlabel('quality')
    axes[int(idx / 3), idx % 3].set_ylabel(feature);

"""Conclusion: majority of features have outliers

## Data pre-processing
"""

# Wine quality classes are disbalanced. To balance them we will concatenate data in 2 groups: with low (<=5) and high quality wines (>5)
new=[]
for row in df['quality']:
    if (row<=5):
        val=0
    else:
        val=1
    new.append(val)
df['newquality']=new

sns.countplot(df['newquality'])

# Removing outliers beyond 3 sigma boundaries 
sigma_threshold = 3
low = stats.norm.cdf(-sigma_threshold)
high = stats.norm.cdf(sigma_threshold)
quant_data = df.quantile([low, high])
                         
for name in list(df.drop(labels=['newquality'], axis=1).columns):
    df = df[(df[name] > quant_data.loc[low, name]) & (df[name] < quant_data.loc[high, name])]
print(df.shape)

df.head()

from sklearn.model_selection import train_test_split

# Splitting data to train and test correspodingly. 80% data-train, 20%-test
X=df.drop(['quality','newquality'],axis=1)
Y=df['newquality']
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2, random_state = 42)

X.head()

# Избавимся от смещения
for name in list(X_train.columns):
      X_train.loc[:,name], fit_boxcox = stats.boxcox(X_train.loc[:,name])
      X_test.loc[:,name] = stats.boxcox(X_test.loc[:,name], fit_boxcox)

# I tried different normalization methods, but later on using data  after Z-score application in ML models I achieved better predicted results. 
# Standartisation (mean=0, sd=1)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_X_train_arrey = scaler.fit_transform(X_train)
scaled_X_test_arrey = scaler.transform(X_test)

scaled_X_train = pd.DataFrame(scaled_X_train_arrey, index=X_train.index, columns=X_train.columns)
scaled_X_test = pd.DataFrame(scaled_X_test_arrey, index=X_test.index, columns=X_test.columns)

scaled_X_train.hist(bins=20,figsize=(12,12))

"""## Building regression models"""

def show_best_params(model):
  #The best parameters of the model
  print("Best params:", model.best_params_)
  print("Best cross validaton score", model.best_score_)

summary = { 'Model' : [],
            'Accuracy' : [],
            'Precision' : [],
            'Recall' : [],
            'ROC_AUC' : []
          }
def create_summary(Y_predict, model_name):
    summary['Model'].append(model_name)
    summary['Accuracy'].append(metrics.accuracy_score(Y_test, Y_predict))
    summary['Precision'].append(metrics.precision_score(Y_test, Y_predict))
    summary['Recall'].append(metrics.recall_score(Y_test, Y_predict))
    summary['ROC_AUC'].append(metrics.roc_auc_score(Y_test, Y_predict))

    print("Accuracy: ", metrics.accuracy_score(Y_test, Y_predict))
    print("Precision: ", metrics.precision_score(Y_test, Y_predict))
    print("Recall: ", metrics.recall_score(Y_test, Y_predict))
    print("ROC AUC: ", metrics.roc_auc_score(Y_test, Y_predict))

def create_ROC_curve(model):
  probs = model.best_estimator_.predict_proba(scaled_X_test)
  preds = probs[:,1]

  fpr, tpr, thresholds = metrics.roc_curve(Y_test, preds)

  plt.plot(fpr, tpr)
  plt.xlim([-0.1, 1.1])
  plt.ylim([-0.1, 1.1])
  plt.rcParams['font.size'] = 12
  plt.title('ROC curve')
  plt.xlabel('False Positive Rate (1 - Specificity)')
  plt.ylabel('True Positive Rate (Sensitivity)')
  plt.grid(True)

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

log_rm_param_grid = {'C': [0.001,0.001,0.1,1,2,3,4,5]
                  }  
cv = ShuffleSplit(n_splits=5, random_state=0)
model = LogisticRegression(random_state=0)
log_rm_grid_search = GridSearchCV(model, log_rm_param_grid, cv=cv, n_jobs=-1)  
log_rm_grid_search.fit(scaled_X_train, Y_train)
log_rm_predict_Y = log_rm_grid_search.best_estimator_.predict(scaled_X_test)

show_best_params(log_rm_grid_search)

create_summary(log_rm_predict_Y, 'Logistic Regression')

create_ROC_curve(log_rm_grid_search)

"""### SVM (Support Vector Machines)"""

from sklearn.svm import SVC

# defining parameter range 
svm_param_grid = {'C': [10,9,8,7,6,5,2,1,0.1,0.01]}  

cv = ShuffleSplit(n_splits=5, random_state=0)
model = SVC(gamma = 0.1, kernel='rbf', probability=True, random_state=0)

svm_grid_search = GridSearchCV(model, param_grid=svm_param_grid, cv=cv, n_jobs=-1)  
svm_grid_search.fit(scaled_X_train, Y_train)
svm_Y_predicted = svm_grid_search.best_estimator_.predict(scaled_X_test)

show_best_params(svm_grid_search)

create_summary(svm_Y_predicted, 'CVM')

create_ROC_curve(svm_grid_search)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

RF_param_grid = {'max_depth': list(range(10,20))
                 }
cv = ShuffleSplit(n_splits=5, random_state=0)
model = RandomForestClassifier(n_estimators=500, min_samples_leaf=1, min_samples_split=2, random_state=0, n_jobs=-1)
RF_grid_search = GridSearchCV(model, RF_param_grid, verbose=True, n_jobs=-1, cv=cv)
RF_grid_search.fit(scaled_X_train, Y_train)
RF_Y_predicted = RF_grid_search.best_estimator_.predict(scaled_X_test)

show_best_params(RF_grid_search)

# feature importances
pd.DataFrame({'Feature' : list(X_train.columns), 'Importance' : RF_grid_search.best_estimator_.feature_importances_}).sort_values(["Importance"], ascending=False)

create_summary(RF_Y_predicted, 'Random Forest')

create_ROC_curve(RF_grid_search)

"""### XGradient Boosting

https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn
"""

import xgboost
from xgboost import XGBClassifier

XGB_grid_param = {'n_estimators' : np.arange(0, 200, 50),
                 }

cv = ShuffleSplit(n_splits=5, random_state=0)
model = XGBClassifier(max_depth=20, learning_rate=0.1, eval_metric = 'logloss', verbose=True, random_state=0)
XGB_grid_search=GridSearchCV(estimator=model, param_grid=XGB_grid_param, n_jobs=-1, cv=cv)

XGB_grid_search.fit(scaled_X_train, Y_train)
XGB_Y_predicted = GB_grid_search.best_estimator_.predict(scaled_X_test)

show_best_params(XGB_grid_search)

pd.DataFrame({'Feature' : list(X_train.columns), 'Importance' : XGB_grid_search.best_estimator_.feature_importances_}).sort_values(["Importance"], ascending=False)

create_summary(XGB_Y_predicted, 'XGradient Boosting')

create_ROC_curve(XGB_grid_search)

"""### Neural Network"""

from keras.models import Sequential
from keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from keras.wrappers.scikit_learn import KerasClassifier

def create_model(learning_rate = 0.01):  
# create model
    model = Sequential()
    model.add(Dense(7, input_dim=11, activation='relu'))  
    model.add(Dense(3, input_dim=7, activation='relu')) 
    model.add(Dense(1, activation='sigmoid'))
    # Compile model
    model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
    return model

# create model
cv = ShuffleSplit(n_splits=5, random_state=0)
model = KerasClassifier(build_fn=create_model, epochs=500, verbose=0)  

#define the grid search parameters
NN_param_grid = {'batch_size' : [30,40,50,60],
                }
NN_grid_search = GridSearchCV(estimator=model, param_grid=NN_param_grid, cv=cv, n_jobs=-1)
NN_grid_search.fit(scaled_X_train, Y_train)
NN_Y_predict = NN_grid_search.best_estimator_.predict(scaled_X_test)

show_best_params(NN_grid_search)

create_summary(NN_Y_predict, 'Neural Network')

create_ROC_curve(NN_grid_search)

"""### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(scaled_X_train, Y_train)
gnb_Y_predicted = gausnb.predict(scaled_X_test)

create_summary(gnb_Y_predicted, 'Naive Bayes')

probs = gnb.predict_proba(scaled_X_test)
preds = probs[:,1]

fpr, tpr, thresholds = metrics.roc_curve(Y_test, preds)

plt.plot(fpr, tpr)
plt.xlim([-0.1, 1.1])
plt.ylim([-0.1, 1.1])
plt.rcParams['font.size'] = 12
plt.title('ROC curve')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.grid(True)

"""## Summary  Table"""

pd.DataFrame(summary)